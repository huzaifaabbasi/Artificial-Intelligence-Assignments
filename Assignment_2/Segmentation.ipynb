{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation of Image into foreground & background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_WIDTH = 384\n",
    "IMG_HEIGHT = 384\n",
    "IMG_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resized images keeping the Aspect ratio and padding with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(input_folder, output_folder, model_input_size):\n",
    "    for file in os.listdir(input_folder):\n",
    "        image = cv2.imread(os.path.join(input_folder,file))\n",
    "        height, width = image.shape[:2]\n",
    "        if model_input_size[0] / height < model_input_size[1] / width:\n",
    "            scale = model_input_size[0] / height\n",
    "        else:\n",
    "            scale = model_input_size[1] / width\n",
    "        new_width = int(scale * width)\n",
    "        new_height = int(scale * height)\n",
    "        resized_image = cv2.resize(image, (new_width, new_height))\n",
    "        final_image = np.zeros(model_input_size)\n",
    "        final_image[int((model_input_size[0] - new_height) / 2) : int((model_input_size[0] - new_height) / 2 + new_height), int((model_input_size[1] - new_width) / 2) : int((model_input_size[1] - new_width) / 2 + new_width), : ] = resized_image\n",
    "        cv2.imwrite(os.path.join(output_folder, file), final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_input = 'Original_Data/Ground_truth/gt'\n",
    "gt_output = 'Resized_Data/Ground_truth/gt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize(gt_input, gt_output, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "melanoma_input = 'Original_Data/Melanoma/melanoma'\n",
    "melanoma_output = 'Resized_Data/Melanoma/melanoma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize(melanoma_input, melanoma_output, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_input = 'Original_Data/Others/others'\n",
    "others_output = 'Resized_Data/Others/others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize(others_input, others_output, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for calculation of the Mean_IOU metric.\n",
    "\n",
    "The mean IOU metric is calculated by the mean intersection over union over different thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Coefficient metric and Dice Coefficient loss\n",
    "\n",
    "Dice Coefficient is (2 * |AB|)/(|A| + |B|)  \n",
    "Dice Coefficient loss is 1 - Dice Coefficient  \n",
    "The loss is the sum of Binary crossentropy and Dice Coefficient loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_coef_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet Network to Segment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input([IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "lambda_layer = Lambda(lambda x : x/255) (inputs)\n",
    "c1 = Conv2D(8, (3, 3), activation = 'elu', padding = 'same') (lambda_layer)\n",
    "c1 = Conv2D(8, (3, 3), activation = 'elu', padding = 'same') (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(16, (3, 3), activation = 'elu', padding = 'same') (p1)\n",
    "c2 = Conv2D(16, (3, 3), activation = 'elu', padding = 'same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(32, (3, 3), activation = 'elu', padding = 'same') (p2)\n",
    "c3 = Conv2D(32, (3, 3), activation = 'elu', padding = 'same') (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(64, (3, 3), activation = 'elu', padding = 'same') (p3)\n",
    "c4 = Conv2D(64, (3, 3), activation = 'elu', padding = 'same') (c4)\n",
    "p4 = MaxPooling2D((2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(128, (3, 3), activation = 'elu', padding = 'same') (p4)\n",
    "c5 = Conv2D(128, (3, 3), activation = 'elu', padding = 'same') (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = 'same') (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(64, (3, 3), activation = 'elu', padding = 'same') (u6)\n",
    "c6 = Conv2D(64, (3, 3), activation = 'elu', padding = 'same') (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = 'same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(32, (3, 3), activation = 'elu', padding = 'same') (u7)\n",
    "c7 = Conv2D(32, (3, 3), activation = 'elu', padding = 'same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = 'same') (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(16, (3, 3), activation = 'elu', padding = 'same') (u8)\n",
    "c8 = Conv2D(16, (3, 3), activation = 'elu', padding = 'same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(8, (2, 2), strides = (2, 2), padding = 'same') (c8)\n",
    "u9 = concatenate([u9, c1])\n",
    "c9 = Conv2D(8, (3, 3), activation = 'elu', padding = 'same') (u9)\n",
    "c9 = Conv2D(8, (3, 3), activation = 'elu', padding = 'same') (c9)\n",
    "\n",
    "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs = [inputs], outputs = [outputs])\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = bce_dice_loss, metrics = [mean_iou, dice_coef, 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in os.listdir(melanoma_output):\n",
    "    x_train.append(cv2.imread(os.path.join(melanoma_output,image)))\n",
    "    ground_image = image[:12] + '_segmentation.png'\n",
    "    gt_image = cv2.imread(os.path.join(gt_output,ground_image),0)\n",
    "    gt_image = cv2.threshold(gt_image,127,255,cv2.THRESH_BINARY)[1]\n",
    "    y_train.append(gt_image)\n",
    "    \n",
    "for image in os.listdir(others_output):\n",
    "    x_train.append(cv2.imread(os.path.join(others_output,image)))\n",
    "    ground_image = image[:12] + '_segmentation.png'\n",
    "    gt_image = cv2.imread(os.path.join(gt_output,ground_image),0)\n",
    "    gt_image = cv2.threshold(gt_image,127,255,cv2.THRESH_BINARY)[1]\n",
    "    y_train.append(gt_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train / 255.0\n",
    "y_train = np.expand_dims(y_train,axis = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model_path = 'Trained/segmentor_bce.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(finetune_model_path, monitor='val_loss', save_best_only=True, verbose=2)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/40\n",
      "1800/1800 [==============================] - 162s 90ms/step - loss: 0.8815 - mean_iou: 0.5144 - dice_coef: 0.4422 - acc: 0.8857 - val_loss: 0.8842 - val_mean_iou: 0.5895 - val_dice_coef: 0.5002 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00001: val_loss did not improve\n",
      "Epoch 2/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.7399 - mean_iou: 0.6089 - dice_coef: 0.5465 - acc: 0.8991 - val_loss: 0.9179 - val_mean_iou: 0.6181 - val_dice_coef: 0.4745 - val_acc: 0.8674\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/40\n",
      "1800/1800 [==============================] - 162s 90ms/step - loss: 0.6594 - mean_iou: 0.6275 - dice_coef: 0.6054 - acc: 0.9117 - val_loss: 0.7492 - val_mean_iou: 0.6387 - val_dice_coef: 0.5374 - val_acc: 0.9035\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.5231 - mean_iou: 0.6516 - dice_coef: 0.6978 - acc: 0.9320 - val_loss: 0.4258 - val_mean_iou: 0.6652 - val_dice_coef: 0.7476 - val_acc: 0.9465\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.4198 - mean_iou: 0.6784 - dice_coef: 0.7553 - acc: 0.9456 - val_loss: 0.4148 - val_mean_iou: 0.6900 - val_dice_coef: 0.7509 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.3845 - mean_iou: 0.7002 - dice_coef: 0.7743 - acc: 0.9492 - val_loss: 0.3551 - val_mean_iou: 0.7085 - val_dice_coef: 0.7661 - val_acc: 0.9559\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.3650 - mean_iou: 0.7164 - dice_coef: 0.7820 - acc: 0.9525 - val_loss: 0.3468 - val_mean_iou: 0.7231 - val_dice_coef: 0.7843 - val_acc: 0.9526\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/40\n",
      "1800/1800 [==============================] - 159s 88ms/step - loss: 0.3375 - mean_iou: 0.7299 - dice_coef: 0.7994 - acc: 0.9559 - val_loss: 0.3229 - val_mean_iou: 0.7355 - val_dice_coef: 0.7891 - val_acc: 0.9614\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.3235 - mean_iou: 0.7411 - dice_coef: 0.8081 - acc: 0.9576 - val_loss: 0.3162 - val_mean_iou: 0.7459 - val_dice_coef: 0.8049 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.3157 - mean_iou: 0.7506 - dice_coef: 0.8126 - acc: 0.9583 - val_loss: 0.3491 - val_mean_iou: 0.7545 - val_dice_coef: 0.7648 - val_acc: 0.9584\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.3110 - mean_iou: 0.7584 - dice_coef: 0.8156 - acc: 0.9593 - val_loss: 0.4345 - val_mean_iou: 0.7617 - val_dice_coef: 0.7794 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.2926 - mean_iou: 0.7650 - dice_coef: 0.8285 - acc: 0.9611 - val_loss: 0.2889 - val_mean_iou: 0.7681 - val_dice_coef: 0.8133 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.2919 - mean_iou: 0.7712 - dice_coef: 0.8272 - acc: 0.9616 - val_loss: 0.2679 - val_mean_iou: 0.7740 - val_dice_coef: 0.8321 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/40\n",
      "1800/1800 [==============================] - 157s 87ms/step - loss: 0.2780 - mean_iou: 0.7769 - dice_coef: 0.8359 - acc: 0.9629 - val_loss: 0.2630 - val_mean_iou: 0.7795 - val_dice_coef: 0.8283 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.2749 - mean_iou: 0.7821 - dice_coef: 0.8370 - acc: 0.9636 - val_loss: 0.2917 - val_mean_iou: 0.7842 - val_dice_coef: 0.8149 - val_acc: 0.9675\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/40\n",
      "1800/1800 [==============================] - 160s 89ms/step - loss: 0.2699 - mean_iou: 0.7865 - dice_coef: 0.8401 - acc: 0.9644 - val_loss: 0.2433 - val_mean_iou: 0.7885 - val_dice_coef: 0.8448 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.2586 - mean_iou: 0.7907 - dice_coef: 0.8469 - acc: 0.9652 - val_loss: 0.3366 - val_mean_iou: 0.7925 - val_dice_coef: 0.8130 - val_acc: 0.9613\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.2581 - mean_iou: 0.7943 - dice_coef: 0.8475 - acc: 0.9654 - val_loss: 0.3035 - val_mean_iou: 0.7960 - val_dice_coef: 0.8179 - val_acc: 0.9611\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.2683 - mean_iou: 0.7977 - dice_coef: 0.8406 - acc: 0.9646 - val_loss: 0.2323 - val_mean_iou: 0.7992 - val_dice_coef: 0.8532 - val_acc: 0.9722\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/40\n",
      "1800/1800 [==============================] - 158s 88ms/step - loss: 0.2554 - mean_iou: 0.8007 - dice_coef: 0.8484 - acc: 0.9662 - val_loss: 0.2356 - val_mean_iou: 0.8023 - val_dice_coef: 0.8529 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/40\n",
      " 464/1800 [======>.......................] - ETA: 1:48 - loss: 0.2538 - mean_iou: 0.8029 - dice_coef: 0.8494 - acc: 0.9666"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-119d37a18bb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = model.fit(x_train, y_train, validation_split=0.1, batch_size=4, epochs=40, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean_IOU = 0.8023\n",
    "\n",
    "### Dice Coefficient = 0.8529\n",
    "\n",
    "### Accuracy = 0.9696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
