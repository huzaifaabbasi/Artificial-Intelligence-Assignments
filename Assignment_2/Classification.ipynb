{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanoma Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\huzai\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import merge\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from skimage.transform import rescale, resize\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp(I):\n",
    "    I = np.uint8(I)\n",
    "    cv2.imshow('image', I)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale = 1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = 'Resized_Data/Ground_truth/gt'\n",
    "melanoma_input = 'Resized_Data/Melanoma/melanoma'\n",
    "melanoma_output_training = 'Partitioned_Data/Training/1_Melanoma'\n",
    "melanoma_output_validation = 'Partitioned_Data/Validation/1_Melanoma'\n",
    "others_input = 'Resized_Data/Others/others'\n",
    "others_output_training = 'Partitioned_Data/Training/0_Others'\n",
    "others_output_validation = 'Partitioned_Data/Validation/0_Others'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Resize and Partitioning\n",
    "\n",
    "Image is multiplied with the ground truth to give the region of interest and then resized the input size of the CNN ( 64 X 64 ).\n",
    "It is partitioned 90:10 ratio between training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "melanoma_length = len(os.listdir(melanoma_input))\n",
    "for image_name in os.listdir(melanoma_input):\n",
    "    counter = counter + 1\n",
    "    image = cv2.imread(os.path.join(melanoma_input,image_name))\n",
    "    ground_image_name = image_name[:12] + '_segmentation.png'\n",
    "    gt_image = cv2.imread(os.path.join(gt_path,ground_image_name))\n",
    "    gt_image = cv2.threshold(gt_image,127,255,cv2.THRESH_BINARY)[1]\n",
    "    gt_image = gt_image / 255.0\n",
    "    image = image * gt_image\n",
    "    image = cv2.resize(image, (64,64))\n",
    "    if counter <= 0.9 * melanoma_length:\n",
    "        cv2.imwrite(os.path.join(melanoma_output_training, image_name), image)\n",
    "    else:\n",
    "        cv2.imwrite(os.path.join(melanoma_output_validation, image_name), image)\n",
    "    \n",
    "counter = 0\n",
    "others_length = len(os.listdir(others_input))\n",
    "for image_name in os.listdir(others_input):\n",
    "    counter = counter + 1\n",
    "    image = cv2.imread(os.path.join(others_input,image_name))\n",
    "    ground_image_name = image_name[:12] + '_segmentation.png'\n",
    "    gt_image = cv2.imread(os.path.join(gt_path,ground_image_name))\n",
    "    gt_image = cv2.threshold(gt_image,127,255,cv2.THRESH_BINARY)[1]\n",
    "    gt_image = gt_image / 255.0\n",
    "    image = image * gt_image\n",
    "    image = cv2.resize(image, (64,64))\n",
    "    if counter <= 0.9 * others_length:\n",
    "        cv2.imwrite(os.path.join(others_output_training, image_name), image)\n",
    "    else:\n",
    "        cv2.imwrite(os.path.join(others_output_validation, image_name), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_input_size = [64, 64, 3]\n",
    "trained_classifier_path = 'Trained/classifier.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-loss\n",
    "This loss was designed to directly maximize the f1-score.  \n",
    "Using this loss did not give much improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "    TP = K.sum(y_true * y_pred)\n",
    "    soft_precision_positive = TP / (K.sum(y_pred) + 1e-7)\n",
    "    soft_recall_positive = TP / (K.sum(y_true) + 1e-7)\n",
    "    f1_loss_positive = 2 * soft_precision_positive * soft_recall_positive / (soft_precision_positive + soft_recall_positive + 1e-7)\n",
    "    TN = K.sum((1 - y_true) * (1 - y_pred))\n",
    "    soft_precision_negative = TN / (K.sum(1 - y_pred) + 1e-7)\n",
    "    soft_recall_negative = TN / (K.sum(1 - y_true) + 1e-7)\n",
    "    f1_loss_negative = 2 * soft_precision_negative * soft_recall_negative / (soft_precision_negative + soft_recall_negative + 1e-7)\n",
    "    return 1 - (0.99 * f1_loss_positive + 0.01 * f1_loss_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Binary Cross-Entropy\n",
    "This loss was designed to balance the skewness of the dataset towards the non-melanoma images.  \n",
    "This loss had been used but did not give much improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    return -K.sum(0.99 * y_true * K.log(y_pred + 1e-7) + 0.01 * (1 - y_true) * K.log(1 - y_pred + 1e-7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 8, 8, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 94,433\n",
      "Trainable params: 94,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv1'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv2'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv3'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), input_shape = CNN_input_size, activation = 'relu', padding = 'same', name = 'conv4'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu', name = 'dense_1'))\n",
    "model.add(Dense(1, name = 'dense_2'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ModelCheckpoint(trained_classifier_path, monitor = 'val_loss', save_best_only = True, verbose = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1799 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n",
      "Epoch 1/18\n",
      "450/450 [==============================] - 10s 23ms/step - loss: 1.5301 - acc: 0.5591 - val_loss: 0.7828 - val_acc: 0.2438\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78284, saving model to Trained/classifier.h5\n",
      "Epoch 2/18\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 1.4848 - acc: 0.6281 - val_loss: 0.8286 - val_acc: 0.2786\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 1.4441 - acc: 0.6564 - val_loss: 0.8038 - val_acc: 0.2537\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 1.3670 - acc: 0.7134 - val_loss: 0.7792 - val_acc: 0.3831\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.78284 to 0.77920, saving model to Trained/classifier.h5\n",
      "Epoch 5/18\n",
      "450/450 [==============================] - 9s 21ms/step - loss: 1.2981 - acc: 0.7481 - val_loss: 0.8915 - val_acc: 0.3284\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/18\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 1.1377 - acc: 0.8092 - val_loss: 0.8358 - val_acc: 0.4776\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 1.0709 - acc: 0.8341 - val_loss: 0.7419 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.77920 to 0.74190, saving model to Trained/classifier.h5\n",
      "Epoch 8/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 1.0628 - acc: 0.8403 - val_loss: 0.9565 - val_acc: 0.4378\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 0.9325 - acc: 0.8737 - val_loss: 0.8573 - val_acc: 0.5323\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/18\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 0.9088 - acc: 0.8822 - val_loss: 0.7073 - val_acc: 0.6716\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.74190 to 0.70731, saving model to Trained/classifier.h5\n",
      "Epoch 11/18\n",
      "450/450 [==============================] - 9s 21ms/step - loss: 0.9049 - acc: 0.8839 - val_loss: 1.0688 - val_acc: 0.5871\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 0.9248 - acc: 0.8790 - val_loss: 0.8058 - val_acc: 0.6567\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 0.7916 - acc: 0.9094 - val_loss: 0.9075 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/18\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 0.8030 - acc: 0.9061 - val_loss: 0.9765 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/18\n",
      "450/450 [==============================] - 9s 21ms/step - loss: 0.7818 - acc: 0.9095 - val_loss: 1.0108 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/18\n",
      "450/450 [==============================] - 10s 22ms/step - loss: 0.8284 - acc: 0.9014 - val_loss: 1.0478 - val_acc: 0.6915\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 0.7913 - acc: 0.9067 - val_loss: 1.0788 - val_acc: 0.6965\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/18\n",
      "450/450 [==============================] - 10s 21ms/step - loss: 0.7972 - acc: 0.9074 - val_loss: 1.1114 - val_acc: 0.6866\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25546a6a0b8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow_from_directory('Partitioned_Data/Training', batch_size = 16, \n",
    "                    class_mode = 'binary', target_size = (64, 64)), steps_per_epoch = 1800 / 4, epochs = 18, \n",
    "                    verbose = 1, validation_data = datagen.flow_from_directory('Partitioned_Data/Validation', \n",
    "                    batch_size = 4, class_mode = 'binary',  target_size = (64, 64)), callbacks = callbacks, class_weight = {0:1, 1:8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = datagen.flow_from_directory('Partitioned_Data/Validation', target_size = (64, 64), class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[127  36]\n",
      " [ 27  11]]\n"
     ]
    }
   ],
   "source": [
    "results = model.predict_generator(test_set)\n",
    "y_true = test_set.classes\n",
    "y_pred = np.rint(results)\n",
    "cm = confusion_matrix(y_true,y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for melanoma: 0.2588235294117647\n",
      "F1 score for non-melanoma: 0.8012618296529967\n"
     ]
    }
   ],
   "source": [
    "# Class - Melanoma\n",
    "precision = cm[1][1] / (cm[1][1] + cm[0][1])\n",
    "recall =  cm[1][1] / (cm[1][1] + cm[1][0])\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 score for melanoma:\", f1)\n",
    "\n",
    "# Class - Others\n",
    "precision = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "recall = cm[0][0] / (cm[0][0] + cm[1][0])\n",
    "\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(\"F1 score for non-melanoma:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melonoma f1-score : 0.258\n",
    "### Other f1-score : 0.801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
